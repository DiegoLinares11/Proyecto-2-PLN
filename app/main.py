import torch
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import load_models as lm

# Configuraci√≥n de p√°gina
st.set_page_config(
    page_title="Clasificador de Argumentos",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Paleta de colores mejorada - M√°ximo contraste y legibilidad
COLORS = {
    'primary': '#1E40AF',      # Azul oscuro vibrante
    'secondary': '#7C3AED',    # P√∫rpura
    'accent': '#F59E0B',       # √Åmbar
    'success': '#059669',      # Verde oscuro
    'warning': '#EA580C',      # Naranja oscuro
    'danger': '#DC2626',       # Rojo oscuro
    'effective': '#059669',    # Verde oscuro para Effective
    'adequate': '#2563EB',     # Azul fuerte para Adequate
    'ineffective': '#EA580C',  # Naranja oscuro para Ineffective
    'background': '#FFFFFF',   # Blanco puro
    'text': '#0F172A',         # Negro azulado
    'text_secondary': '#1E293B', # Gris muy oscuro (en lugar de gris claro)
    'card_bg': '#F8FAFC'       # Gris muy claro para cards
}

# CSS personalizado
st.markdown(f"""
    <style>
    /* Fondo principal */
    .stApp {{
        background-color: {COLORS['background']};
    }}
    
    /* T√≠tulos */
    h1 {{
        color: {COLORS['primary']};
        font-weight: 700;
        padding-bottom: 0.5rem;
        border-bottom: 4px solid {COLORS['accent']};
        margin-bottom: 1.5rem;
    }}
    
    h2 {{
        color: {COLORS['secondary']};
        font-weight: 600;
    }}
    
    h3 {{
        color: {COLORS['primary']};
        font-weight: 500;
    }}
    
    h4 {{
        color: {COLORS['text']};
        font-weight: 600;
    }}
    
    /* Todo el texto debe ser oscuro y visible */
    p, span, div, label {{
        color: {COLORS['text']} !important;
    }}
    
    /* Texto secundario con mejor contraste */
    .stMarkdown p {{
        color: {COLORS['text_secondary']} !important;
    }}
    
    /* Botones */
    .stButton>button {{
        background-color: {COLORS['primary']};
        color: white !important;
        font-weight: 600;
        border-radius: 10px;
        border: none;
        padding: 0.6rem 2rem;
        transition: all 0.3s ease;
    }}
    
    .stButton>button:hover {{
        background-color: {COLORS['secondary']};
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(0,0,0,0.2);
    }}
    
    /* Selectbox y inputs con texto oscuro */
    .stSelectbox label, .stTextArea label {{
        color: {COLORS['text']} !important;
        font-weight: 600;
    }}
    
    /* Fix dropdown options visibility */
    .stSelectbox > div > div {{
        background-color: white !important;
        color: {COLORS['text']} !important;
    }}
    
    .stSelectbox [data-baseweb="select"] {{
        background-color: white !important;
    }}
    
    .stSelectbox [data-baseweb="select"] > div {{
        background-color: white !important;
        color: {COLORS['text']} !important;
    }}
    
    /* Dropdown menu */
    [role="listbox"] {{
        background-color: white !important;
    }}
    
    [role="option"] {{
        background-color: white !important;
        color: {COLORS['text']} !important;
    }}
    
    [role="option"]:hover {{
        background-color: {COLORS['card_bg']} !important;
        color: {COLORS['primary']} !important;
    }}
    
    /* M√©tricas */
    [data-testid="stMetricValue"] {{
        font-size: 2.2rem;
        font-weight: 700;
        color: {COLORS['primary']};
    }}
    
    [data-testid="stMetricLabel"] {{
        color: {COLORS['text']} !important;
        font-weight: 600;
    }}
    
    [data-testid="stMetricDelta"] {{
        color: {COLORS['success']} !important;
    }}
    
    /* Sidebar */
    [data-testid="stSidebar"] {{
        background-color: #E5E7EB;
        border-right: 3px solid {COLORS['primary']};
    }}
    
    [data-testid="stSidebar"] * {{
        color: {COLORS['text']} !important;
    }}
    
    [data-testid="stSidebar"] h1,
    [data-testid="stSidebar"] h2,
    [data-testid="stSidebar"] h3 {{
        color: {COLORS['primary']} !important;
    }}
    
    /* Radio buttons en sidebar */
    [data-testid="stSidebar"] [role="radiogroup"] label {{
        color: {COLORS['text']} !important;
        font-weight: 600;
        font-size: 1rem;
    }}
    
    [data-testid="stSidebar"] [role="radiogroup"] [data-checked="true"] {{
        background-color: {COLORS['primary']} !important;
    }}
    
    [data-testid="stSidebar"] [role="radiogroup"] [data-checked="true"] * {{
        color: white !important;
    }}
    
    /* Cards personalizados */
    .metric-card {{
        background: white;
        padding: 1.8rem;
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        border-left: 5px solid {COLORS['primary']};
        margin: 1rem 0;
        transition: transform 0.2s ease;
    }}
    
    .metric-card:hover {{
        transform: translateY(-4px);
        box-shadow: 0 8px 20px rgba(0,0,0,0.12);
    }}
    
    .metric-card p {{
        color: {COLORS['text']} !important;
    }}
    
    /* Tabs */
    .stTabs [data-baseweb="tab-list"] {{
        gap: 2rem;
        background-color: {COLORS['card_bg']};
        padding: 0.5rem;
        border-radius: 10px;
    }}
    
    .stTabs [data-baseweb="tab"] {{
        font-weight: 600;
        font-size: 1.1rem;
        color: {COLORS['text']} !important;
    }}
    
    .stTabs [aria-selected="true"] {{
        background-color: {COLORS['primary']} !important;
        color: white !important;
        border-radius: 8px;
    }}
    
    /* Info boxes con texto oscuro */
    .info-box {{
        background: linear-gradient(135deg, {COLORS['primary']}10 0%, {COLORS['accent']}10 100%);
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid {COLORS['accent']};
        margin: 1rem 0;
    }}
    
    .info-box * {{
        color: {COLORS['text']} !important;
    }}
    
    /* Radio buttons */
    [data-testid="stRadio"] label {{
        font-weight: 600;
        color: {COLORS['text']} !important;
    }}
    
    /* Dataframe */
    .stDataFrame {{
        color: {COLORS['text']} !important;
    }}
    
    /* Expander */
    .streamlit-expanderHeader {{
        color: {COLORS['text']} !important;
        font-weight: 600;
    }}
    
    /* Success/Info/Warning/Error boxes */
    .stAlert {{
        color: {COLORS['text']} !important;
    }}
    </style>
""", unsafe_allow_html=True)

# Device setup
device = "cpu"

def evaluate_models_safe(device, distilbert_model, deberta_model, svm_model, tfidf_vectorizer, use_train=False, sample_size=100):
    """Versi√≥n segura con muestreo y progreso visible"""
    
    if use_train:
        from sklearn.model_selection import train_test_split
        full_dataset = pd.read_csv("../data/clean_train.csv")
        
        # Detectar columna de texto
        if 'discourse_text_clean' in full_dataset.columns:
            text_col = 'discourse_text_clean'
        elif 'discourse_text' in full_dataset.columns:
            text_col = 'discourse_text'
        else:
            text_cols = [col for col in full_dataset.columns if 'text' in col.lower() and 'essay' not in col.lower()]
            text_col = text_cols[0] if text_cols else None
        
        if not text_col or 'label' not in full_dataset.columns:
            st.error("No se pudo preparar el dataset para evaluaci√≥n")
            return None
        
        _, test_subset = train_test_split(
            full_dataset, 
            test_size=0.2, 
            random_state=42, 
            stratify=full_dataset['label']
        )
        
        # SAMPLE SUBSET TO REDUCE TIME
        if len(test_subset) > sample_size:
            test_subset = test_subset.sample(n=sample_size, random_state=42)
        
        X_test = test_subset[text_col]
        y_true = test_subset['label']
        
        st.info(f"‚úÖ Evaluando con {len(test_subset)} muestras del train set (20% para validaci√≥n)")
        
    else:
        test_dataset = pd.read_csv("../data/clean_test.csv")
        
        if 'discourse_text_clean' in test_dataset.columns:
            text_col = 'discourse_text_clean'
        elif 'discourse_text' in test_dataset.columns:
            text_col = 'discourse_text'
        else:
            text_cols = [col for col in test_dataset.columns if 'text' in col.lower() and 'essay' not in col.lower()]
            if text_cols:
                text_col = text_cols[0]
            else:
                st.error("No se encontr√≥ columna de texto en test dataset")
                return None
        
        if 'label' not in test_dataset.columns:
            st.error("El dataset de prueba no tiene la columna 'label'.")
            return None
        
        # SAMPLE SUBSET TO REDUCE TIME
        if len(test_dataset) > sample_size:
            test_dataset = test_dataset.sample(n=sample_size, random_state=42)
        
        X_test = test_dataset[text_col]
        y_true = test_dataset["label"]
        
        st.info(f"‚úÖ Evaluando con {len(test_dataset)} muestras del test set")
    
    # Define label mappings
    label_map = {0: 'Ineffective', 1: 'Adequate', 2: 'Effective'}
    
    # Convert y_true to string labels if they're numeric
    if y_true.dtype in ['int64', 'int32', 'float64']:
        y_true_mapped = y_true.map(label_map)
    else:
        y_true_mapped = y_true
    
    results = {
        "Model": [],
        "Accuracy": [],
        "Precision": [],
        "Recall": [],
        "F1-Score": []
    }
    
    # Progress bar
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Evaluate DistilBERT
    status_text.text("üîÑ Evaluando DistilBERT...")
    y_pred = []
    for i, text in enumerate(X_test):
        pred = lm.predict_distilbert(text, device, distilbert_model)
        y_pred.append(pred)
        if i % 10 == 0:
            progress_bar.progress((i / len(X_test)) * 0.33)
    
    # Convert numeric predictions to string labels
    y_pred_mapped = [label_map[p] for p in y_pred]
    
    results["Model"].append("DistilBERT")
    results["Accuracy"].append(accuracy_score(y_true_mapped, y_pred_mapped))
    results["Precision"].append(precision_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["Recall"].append(recall_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["F1-Score"].append(f1_score(y_true_mapped, y_pred_mapped, average="macro"))
    
    # Evaluate DeBERTa
    status_text.text("üîÑ Evaluando DeBERTa...")
    y_pred = []
    for i, text in enumerate(X_test):
        pred = lm.predict_deberta(text, device, deberta_model)
        y_pred.append(pred)
        if i % 10 == 0:
            progress_bar.progress(0.33 + (i / len(X_test)) * 0.33)
    
    # DeBERTa already returns string labels
    y_pred_mapped = y_pred
    
    results["Model"].append("DeBERTa")
    results["Accuracy"].append(accuracy_score(y_true_mapped, y_pred_mapped))
    results["Precision"].append(precision_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["Recall"].append(recall_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["F1-Score"].append(f1_score(y_true_mapped, y_pred_mapped, average="macro"))
    
    # Evaluate SVM
    status_text.text("üîÑ Evaluando SVM + TF-IDF...")
    y_pred = []
    for i, text in enumerate(X_test):
        pred = lm.predict_svm(text, svm_model, tfidf_vectorizer)
        y_pred.append(pred)
        if i % 10 == 0:
            progress_bar.progress(0.66 + (i / len(X_test)) * 0.34)
    
    # Convert numeric predictions to string labels - handle numpy integers
    y_pred_mapped = []
    for p in y_pred:
        if isinstance(p, str):
            y_pred_mapped.append(p)
        else:
            # Convert to Python int to handle numpy types
            y_pred_mapped.append(label_map[int(p)])
    
    results["Model"].append("SVM + TF-IDF")
    results["Accuracy"].append(accuracy_score(y_true_mapped, y_pred_mapped))
    results["Precision"].append(precision_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["Recall"].append(recall_score(y_true_mapped, y_pred_mapped, average="macro"))
    results["F1-Score"].append(f1_score(y_true_mapped, y_pred_mapped, average="macro"))
    
    progress_bar.progress(1.0)
    status_text.text("‚úÖ Evaluaci√≥n completada!")
    
    df = pd.DataFrame(results)
    return df

# Cargar modelos en cach√©
@st.cache_resource
def load_all_models():
    """Carga todos los modelos una sola vez"""
    with st.spinner("‚è≥ Cargando modelos..."):
        distilbert = lm.load_distilbert(device)
        deberta = lm.load_deberta(device)
        svm, tfidf = lm.load_svm()
    return distilbert, deberta, svm, tfidf

# Cargar datos en cach√©
@st.cache_data
def load_datasets():
    """Carga los datasets de entrenamiento y prueba"""
    try:
        train_df = pd.read_csv("../data/clean_train.csv")
        test_df = pd.read_csv("../data/clean_test.csv")
    except FileNotFoundError:
        st.error("‚ùå No se encontraron los archivos de datos. Verifica que est√©n en ../data/")
        st.stop()
    
    # Identificar columna de texto en TRAIN
    if 'discourse_text_clean' in train_df.columns:
        train_text_col = 'discourse_text_clean'
    elif 'discourse_text' in train_df.columns:
        train_text_col = 'discourse_text'
    else:
        train_text_cols = [col for col in train_df.columns if 'text' in col.lower() and 'essay' not in col.lower()]
        if train_text_cols:
            train_text_col = train_text_cols[0]
        else:
            st.error(f"‚ùå No se encontr√≥ columna de texto en train. Columnas: {train_df.columns.tolist()}")
            st.stop()
    
    # Identificar columna de texto en TEST
    if 'discourse_text_clean' in test_df.columns:
        test_text_col = 'discourse_text_clean'
    elif 'discourse_text' in test_df.columns:
        test_text_col = 'discourse_text'
    else:
        test_text_cols = [col for col in test_df.columns if 'text' in col.lower() and 'essay' not in col.lower()]
        if test_text_cols:
            test_text_col = test_text_cols[0]
        else:
            test_text_col = None  # Test puede no tener texto
    
    st.sidebar.info(f"üìù Train: `{train_text_col}`\n\nüìù Test: `{test_text_col if test_text_col else 'N/A'}`")
    
    # Mapeo de labels
    label_map = {0: 'Ineffective', 1: 'Adequate', 2: 'Effective'}
    
    if 'label' in train_df.columns:
        train_df['label_name'] = train_df['label'].map(label_map)
    else:
        st.error("‚ùå No se encontr√≥ la columna 'label' en el dataset de entrenamiento")
        st.stop()
    
    # Solo mapear label_name si existe la columna 'label' en test
    if 'label' in test_df.columns:
        test_df['label_name'] = test_df['label'].map(label_map)
    
    # Calcular longitud de textos SOLO si existen las columnas
    if train_text_col in train_df.columns:
        train_df['text_length'] = train_df[train_text_col].str.len()
        train_df['word_count'] = train_df[train_text_col].str.split().str.len()
    
    if test_text_col and test_text_col in test_df.columns:
        test_df['text_length'] = test_df[test_text_col].str.len()
        test_df['word_count'] = test_df[test_text_col].str.split().str.len()
    
    return train_df, test_df, train_text_col

# Sidebar - Navegaci√≥n
with st.sidebar:
    st.title("Clasificador de Argumentos")
    st.markdown("---")
    
    # Selector de p√°gina
    page = st.radio(
        "Navegaci√≥n",
        ["üè† Inicio", "üìä Exploraci√≥n de Datos", "ü§ñ Clasificador", "üìà Rendimiento de Modelos"],
        label_visibility="collapsed"
    )
    
    st.markdown("---")
    
    # Informaci√≥n del modelo
    with st.expander("‚ÑπÔ∏è Informaci√≥n"):
        st.markdown("""
        **Modelos disponibles:**
        - DistilBERT
        - DeBERTa
        - SVM + TF-IDF
        
        **Clases:**
        - üü¢ Effective
        - üîµ Adequate
        - üü† Ineffective
        """)

# Cargar modelos y datos
distilbert_model, deberta_model, svm_model, tfidf_vectorizer = load_all_models()
train_df, test_df, text_col = load_datasets()

st.sidebar.success("‚úÖ Modelos cargados correctamente")

# ==================== P√ÅGINA: INICIO ====================
if page == "üè† Inicio":
    st.title("Bienvenido al Clasificador de Argumentos")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: {COLORS['primary']};">üìö Dataset</h3>
            <p style="font-size: 2rem; font-weight: 700; color: {COLORS['accent']};">
                {len(train_df) + len(test_df)}
            </p>
            <p style="color: {COLORS['text_secondary']}; font-weight: 600;">argumentos totales</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: {COLORS['primary']};">ü§ñ Modelos</h3>
            <p style="font-size: 2rem; font-weight: 700; color: {COLORS['accent']};">3</p>
            <p style="color: {COLORS['text_secondary']}; font-weight: 600;">algoritmos disponibles</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: {COLORS['primary']};">üéØ Clases</h3>
            <p style="font-size: 2rem; font-weight: 700; color: {COLORS['accent']};">3</p>
            <p style="color: {COLORS['text_secondary']}; font-weight: 600;">categor√≠as de clasificaci√≥n</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    st.markdown("""
    <div class="info-box">
        <h3>‚ÑπÔ∏è Sobre el Proyecto</h3>
        <p>
        Esta aplicaci√≥n utiliza t√©cnicas de <strong>Procesamiento de Lenguaje Natural (NLP)</strong> 
        para clasificar argumentos en tres categor√≠as: <strong>Effective</strong>, <strong>Adequate</strong>, 
        y <strong>Ineffective</strong>.
        </p>
        <p>
        Puedes explorar los datos, clasificar nuevos argumentos en tiempo real, y comparar el 
        rendimiento de diferentes modelos de Machine Learning y Deep Learning.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("### ‚ú® Funcionalidades")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **üìä Exploraci√≥n de Datos**
        - Visualizaci√≥n de distribuci√≥n de clases
        - An√°lisis de longitud de textos
        - Word clouds interactivos
        - Estad√≠sticas descriptivas
        """)
    
    with col2:
        st.markdown("""
        **ü§ñ Clasificaci√≥n Inteligente**
        - Clasificaci√≥n con 3 modelos diferentes
        - Comparaci√≥n de predicciones
        - An√°lisis de rendimiento
        - M√©tricas detalladas
        """)

# ==================== P√ÅGINA: EXPLORACI√ìN DE DATOS ====================
elif page == "üìä Exploraci√≥n de Datos":
    st.title("üìä Exploraci√≥n de Datos")
    st.markdown("Analiza las caracter√≠sticas y distribuci√≥n del conjunto de datos de entrenamiento")
    
    # Tabs para organizar las visualizaciones
    tab1, tab2, tab3, tab4 = st.tabs(["üìà Distribuci√≥n", "üìè An√°lisis de Texto", "‚òÅÔ∏è Word Clouds", "üî¢ Estad√≠sticas"])
    
    with tab1:
        st.subheader("Distribuci√≥n de Clases")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Gr√°fico de barras interactivo
            class_counts = train_df['label_name'].value_counts().reset_index()
            class_counts.columns = ['Clase', 'Cantidad']
            
            fig = px.bar(
                class_counts,
                x='Clase',
                y='Cantidad',
                color='Clase',
                color_discrete_map={
                    'Effective': COLORS['effective'],
                    'Adequate': COLORS['adequate'],
                    'Ineffective': COLORS['ineffective']
                },
                title="Distribuci√≥n de Clases en el Dataset de Entrenamiento",
                text='Cantidad'
            )
            fig.update_traces(texttemplate='%{text}', textposition='outside')
            fig.update_layout(
                showlegend=False,
                height=400,
                xaxis_title="Clase",
                yaxis_title="N√∫mero de Argumentos"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("### üìä Resumen")
            for label, count in class_counts.itertuples(index=False):
                percentage = (count / len(train_df)) * 100
                st.metric(
                    label=label,
                    value=f"{count}",
                    delta=f"{percentage:.1f}%"
                )
        
        # Gr√°fico de pie
        fig_pie = px.pie(
            class_counts,
            values='Cantidad',
            names='Clase',
            title="Proporci√≥n de Clases",
            color='Clase',
            color_discrete_map={
                'Effective': COLORS['effective'],
                'Adequate': COLORS['adequate'],
                'Ineffective': COLORS['ineffective']
            },
            hole=0.4
        )
        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig_pie, use_container_width=True)
        
        # Distribuci√≥n por tipo de discurso
        st.subheader("Distribuci√≥n por Tipo de Discurso")
        discourse_dist = train_df.groupby(['discourse_type', 'label_name']).size().reset_index(name='count')
        
        fig_discourse = px.bar(
            discourse_dist,
            x='discourse_type',
            y='count',
            color='label_name',
            barmode='group',
            title="Clases por Tipo de Discurso",
            color_discrete_map={
                'Effective': COLORS['effective'],
                'Adequate': COLORS['adequate'],
                'Ineffective': COLORS['ineffective']
            },
            labels={'discourse_type': 'Tipo de Discurso', 'count': 'Cantidad', 'label_name': 'Clase'}
        )
        fig_discourse.update_layout(height=500)
        st.plotly_chart(fig_discourse, use_container_width=True)
    
    with tab2:
        st.subheader("An√°lisis de Longitud de Textos")
        
        # Selector de m√©trica
        metric_choice = st.radio(
            "Selecciona la m√©trica a analizar:",
            ["Longitud de Caracteres", "N√∫mero de Palabras"],
            horizontal=True
        )
        
        metric_col = 'text_length' if metric_choice == "Longitud de Caracteres" else 'word_count'
        metric_label = 'Caracteres' if metric_choice == "Longitud de Caracteres" else 'Palabras'
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Box plot
            fig_box = px.box(
                train_df,
                x='label_name',
                y=metric_col,
                color='label_name',
                title=f"Distribuci√≥n de {metric_label} por Clase",
                color_discrete_map={
                    'Effective': COLORS['effective'],
                    'Adequate': COLORS['adequate'],
                    'Ineffective': COLORS['ineffective']
                },
                labels={'label_name': 'Clase', metric_col: f'N√∫mero de {metric_label}'}
            )
            fig_box.update_layout(showlegend=False, height=400)
            st.plotly_chart(fig_box, use_container_width=True)
        
        with col2:
            # Violin plot
            fig_violin = px.violin(
                train_df,
                x='label_name',
                y=metric_col,
                color='label_name',
                title=f"Densidad de {metric_label} por Clase",
                color_discrete_map={
                    'Effective': COLORS['effective'],
                    'Adequate': COLORS['adequate'],
                    'Ineffective': COLORS['ineffective']
                },
                box=True,
                labels={'label_name': 'Clase', metric_col: f'N√∫mero de {metric_label}'}
            )
            fig_violin.update_layout(showlegend=False, height=400)
            st.plotly_chart(fig_violin, use_container_width=True)
        
        # Histograma interactivo
        fig_hist = px.histogram(
            train_df,
            x=metric_col,
            color='label_name',
            marginal='box',
            title=f"Distribuci√≥n de {metric_label} en todos los Argumentos",
            color_discrete_map={
                'Effective': COLORS['effective'],
                'Adequate': COLORS['adequate'],
                'Ineffective': COLORS['ineffective']
            },
            labels={metric_col: f'N√∫mero de {metric_label}', 'label_name': 'Clase'},
            nbins=50
        )
        fig_hist.update_layout(height=500)
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # Estad√≠sticas por clase
        st.subheader(f"üìä Estad√≠sticas de {metric_label} por Clase")
        stats_df = train_df.groupby('label_name')[metric_col].describe().round(2)
        st.dataframe(stats_df, use_container_width=True)
    
    with tab3:
        st.subheader("‚òÅÔ∏è Nubes de Palabras por Clase")
        
        class_selector = st.selectbox(
            "Selecciona una clase para visualizar:",
            ['Effective', 'Adequate', 'Ineffective']
        )
        
        # Filtrar textos por clase usando la columna correcta
        class_texts = train_df[train_df['label_name'] == class_selector][text_col]
        all_text = ' '.join(class_texts.astype(str))
        
        # Generar word cloud
        wordcloud = WordCloud(
            width=1200,
            height=600,
            background_color='white',
            colormap='viridis',
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10
        ).generate(all_text)
        
        # Mostrar word cloud
        fig, ax = plt.subplots(figsize=(15, 8))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(f'Palabras m√°s frecuentes en argumentos {class_selector}', 
                    fontsize=20, fontweight='bold', pad=20)
        st.pyplot(fig)
        
        # Mostrar top palabras
        st.subheader(f"üìä Top 20 Palabras en {class_selector}")
        
        from collections import Counter
        words = all_text.lower().split()
        word_freq = Counter(words).most_common(20)
        word_df = pd.DataFrame(word_freq, columns=['Palabra', 'Frecuencia'])
        
        fig_words = px.bar(
            word_df,
            x='Frecuencia',
            y='Palabra',
            orientation='h',
            title=f'Top 20 Palabras m√°s Frecuentes - {class_selector}',
            color='Frecuencia',
            color_continuous_scale='viridis'
        )
        fig_words.update_layout(height=600, showlegend=False)
        st.plotly_chart(fig_words, use_container_width=True)
    
    with tab4:
        st.subheader("üî¢ Estad√≠sticas Descriptivas del Dataset")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üìö Dataset de Entrenamiento")
            st.metric("Total de argumentos", len(train_df))
            st.metric("Tipos de discurso √∫nicos", train_df['discourse_type'].nunique())
            st.metric("Promedio de caracteres", f"{train_df['text_length'].mean():.0f}")
            st.metric("Promedio de palabras", f"{train_df['word_count'].mean():.0f}")
        
        with col2:
            st.markdown("### üìù Dataset de Prueba")
            st.metric("Total de argumentos", len(test_df))
            st.metric("Tipos de discurso √∫nicos", test_df['discourse_type'].nunique() if 'discourse_type' in test_df.columns else "N/A")
            
            # Solo mostrar m√©tricas de texto si existen
            if 'text_length' in test_df.columns:
                st.metric("Promedio de caracteres", f"{test_df['text_length'].mean():.0f}")
            if 'word_count' in test_df.columns:
                st.metric("Promedio de palabras", f"{test_df['word_count'].mean():.0f}")
        
        st.markdown("---")
        
        # Tabla de estad√≠sticas completa
        st.subheader("üìã Estad√≠sticas Detalladas - Training Set")
        
        stats_summary = pd.DataFrame({
            'M√©trica': ['Total', 'Effective', 'Adequate', 'Ineffective'],
            'Cantidad Train': [
                len(train_df),
                len(train_df[train_df['label_name'] == 'Effective']),
                len(train_df[train_df['label_name'] == 'Adequate']),
                len(train_df[train_df['label_name'] == 'Ineffective'])
            ],
            '% del Total': [
                100.0,
                (len(train_df[train_df['label_name'] == 'Effective']) / len(train_df)) * 100,
                (len(train_df[train_df['label_name'] == 'Adequate']) / len(train_df)) * 100,
                (len(train_df[train_df['label_name'] == 'Ineffective']) / len(train_df)) * 100
            ]
        })
        
        # Formatear porcentajes
        stats_summary['% del Total'] = stats_summary['% del Total'].apply(lambda x: f"{x:.2f}%")
        
        st.dataframe(stats_summary, use_container_width=True, hide_index=True)
        
        # An√°lisis de texto solo con train
        st.subheader("üìè An√°lisis de Longitud de Texto (Training)")
        
        text_stats = train_df.groupby('label_name')[['text_length', 'word_count']].describe().round(2)
        st.dataframe(text_stats, use_container_width=True)

# ==================== P√ÅGINA: CLASIFICADOR ====================
elif page == "ü§ñ Clasificador":
    st.title("ü§ñ Clasificador de Argumentos")
    st.markdown("Clasifica nuevos argumentos usando los modelos entrenados")
    
    # Selector de modelo
    col1, col2 = st.columns([3, 1])
    
    with col1:
        model_choice = st.selectbox(
            "üîß Selecciona el modelo a utilizar:",
            ("DistilBERT", "DeBERTa", "SVM + TF-IDF", "Todos los Modelos"),
            help="Elige un modelo espec√≠fico o prueba con todos para comparar"
        )
    
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        show_info = st.checkbox("‚ÑπÔ∏è Mostrar info", value=False)
    
    if show_info:
        st.info("""
        **DistilBERT**: Modelo transformer eficiente, balance entre velocidad y precisi√≥n.  
        **DeBERTa**: Modelo transformer avanzado con mejor comprensi√≥n contextual.  
        **SVM + TF-IDF**: Modelo cl√°sico de ML, r√°pido y efectivo para textos.
        """)
    
    st.markdown("---")
    
    # √Årea de texto para el argumento
    argument = st.text_area(
        label="‚úçÔ∏è Escribe tu argumento aqu√≠:",
        placeholder="Ejemplo: Students should be required to study abroad because it broadens their cultural perspectives and helps them develop independence...",
        key="argument_text",
        height=200,
        help="Escribe el argumento que deseas clasificar"
    )
    
    # Bot√≥n de clasificaci√≥n
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        classify_btn = st.button(
            "üöÄ Clasificar Argumento",
            key="classify_btn",
            use_container_width=True,
            type="primary"
        )
    
    if classify_btn:
        if argument.strip() == "":
            st.warning("‚ö†Ô∏è Por favor escribe un argumento antes de clasificar.")
        else:
            with st.spinner("üîÑ Clasificando tu argumento..."):
                results = {}
                
                # Realizar predicciones seg√∫n el modelo seleccionado
                if model_choice in ["DistilBERT", "Todos los Modelos"]:
                    pred = lm.predict_distilbert(argument, device, distilbert_model)
                    label_map = {0: 'Ineffective', 1: 'Adequate', 2: 'Effective'}
                    results["DistilBERT"] = label_map[pred]
                
                if model_choice in ["DeBERTa", "Todos los Modelos"]:
                    pred = lm.predict_deberta(argument, device, deberta_model)
                    results["DeBERTa"] = pred
                
                if model_choice in ["SVM + TF-IDF", "Todos los Modelos"]:
                    pred = lm.predict_svm(argument, svm_model, tfidf_vectorizer)
                    label_map = {0: 'Ineffective', 1: 'Adequate', 2: 'Effective'}
                    results["SVM + TF-IDF"] = label_map[int(pred)]
                
                # Guardar en session state
                st.session_state["last_prediction"] = results
                st.session_state["last_argument"] = argument
            
            st.success("‚úÖ Clasificaci√≥n completada!")
            
            st.markdown("---")
            st.subheader("üìä Resultados de la Clasificaci√≥n")
            
            # Mostrar resultados en columnas
            if len(results) == 1:
                # Un solo modelo
                model_name, prediction = list(results.items())[0]
                
                # Color seg√∫n la predicci√≥n
                if prediction == 'Effective':
                    color = COLORS['effective']
                    icon = "üü¢"
                elif prediction == 'Adequate':
                    color = COLORS['adequate']
                    icon = "üîµ"
                else:
                    color = COLORS['ineffective']
                    icon = "üü†"
                
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, {color}20 0%, {color}10 100%); 
                            padding: 2rem; border-radius: 12px; border-left: 5px solid {color}; text-align: center;">
                    <h2 style="color: {color}; margin: 0;">{icon} {prediction}</h2>
                    <p style="color: {COLORS['text_secondary']}; margin-top: 0.5rem; font-weight: 600;">Predicci√≥n de {model_name}</p>
                </div>
                """, unsafe_allow_html=True)
            
            else:
                # M√∫ltiples modelos
                cols = st.columns(len(results))
                
                for idx, (model_name, prediction) in enumerate(results.items()):
                    with cols[idx]:
                        # Color seg√∫n la predicci√≥n
                        if prediction == 'Effective':
                            color = COLORS['effective']
                            icon = "üü¢"
                        elif prediction == 'Adequate':
                            color = COLORS['adequate']
                            icon = "üîµ"
                        else:
                            color = COLORS['ineffective']
                            icon = "üü†"
                        
                        st.markdown(f"""
                        <div style="background: white; padding: 1.5rem; border-radius: 12px; 
                                    box-shadow: 0 2px 8px rgba(0,0,0,0.1); border-top: 4px solid {color}; 
                                    text-align: center; height: 180px; display: flex; 
                                    flex-direction: column; justify-content: center;">
                            <h4 style="color: {COLORS['text']}; margin: 0;">{model_name}</h4>
                            <h2 style="color: {color}; margin: 0.5rem 0;">{icon}</h2>
                            <h3 style="color: {color}; margin: 0;">{prediction}</h3>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Consenso
                st.markdown("---")
                predictions_list = list(results.values())
                
                if len(set(predictions_list)) == 1:
                    st.success(f"‚úÖ **Consenso total**: Todos los modelos coinciden en que el argumento es **{predictions_list[0]}**")
                else:
                    from collections import Counter
                    most_common = Counter(predictions_list).most_common(1)[0]
                    st.info(f"‚ÑπÔ∏è **Predicci√≥n mayoritaria**: **{most_common[0]}** ({most_common[1]}/{len(predictions_list)} modelos)")
    
    # Mostrar √∫ltimas predicciones
    if "last_prediction" in st.session_state:
        st.markdown("---")
        st.subheader("üïí √öltima Clasificaci√≥n")
        
        with st.expander("Ver argumento clasificado"):
            st.markdown(f"**Texto:** {st.session_state['last_argument']}")
            st.markdown("**Predicciones:**")
            for model_name, pred in st.session_state["last_prediction"].items():
                st.markdown(f"- **{model_name}**: `{pred}`")

# ==================== P√ÅGINA: RENDIMIENTO ====================
elif page == "üìà Rendimiento de Modelos":
    st.title("üìà Rendimiento de Modelos")
    st.markdown("Compara el desempe√±o de los modelos en el conjunto de datos de prueba")
    
    # Verificar si el dataset de prueba tiene labels
    has_labels = 'label' in test_df.columns
    
    if not has_labels:
        st.info("""
        ‚ÑπÔ∏è **El dataset de prueba no contiene labels**
        
        Para evaluar el rendimiento, usaremos una **validaci√≥n cruzada** sobre el dataset de entrenamiento.
        Esto es una pr√°ctica est√°ndar en Machine Learning cuando no tienes un test set con labels.
        
        Se dividir√° el training set en: 80% entrenamiento / 20% validaci√≥n
        """)
        
        # Usar el train set para evaluaci√≥n
        eval_df = train_df.copy()
        use_train_for_eval = True
    else:
        st.success("‚úÖ El dataset de prueba tiene labels. Se usar√° para la evaluaci√≥n.")
        eval_df = test_df.copy()
        use_train_for_eval = False
    
    # Selector de tama√±o de muestra
    col1, col2 = st.columns([2, 1])
    
    with col1:
        sample_size = st.slider(
            "üìä N√∫mero de muestras para evaluar",
            min_value=50,
            max_value=500,
            value=100,
            step=50,
            help="Menos muestras = m√°s r√°pido, pero menos preciso"
        )
    
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        calc_performance = st.button(
            "üöÄ Calcular Rendimiento",
            use_container_width=True,
            type="primary"
        )
    
    if calc_performance or "performance_data" in st.session_state:
        
        if calc_performance:
            with st.spinner("üîÑ Evaluando modelos... (esto puede tomar un minuto)"):
                performance_data = evaluate_models_safe(
                    device, 
                    distilbert_model, 
                    deberta_model, 
                    svm_model, 
                    tfidf_vectorizer,
                    use_train=use_train_for_eval,
                    sample_size=sample_size
                )
                
                if performance_data is None:
                    st.error("‚ùå No se pudo evaluar. Verifica los datos.")
                    st.stop()
                
                st.session_state["performance_data"] = performance_data
        else:
            performance_data = st.session_state["performance_data"]
        
        st.success("‚úÖ Evaluaci√≥n completada!")
        
        # Tabs para diferentes visualizaciones
        tab1, tab2, tab3 = st.tabs(["üìä Comparaci√≥n General", "üéØ M√©tricas Detalladas", "üìã Tabla de Resultados"])
        
        with tab1:
            st.subheader("Comparaci√≥n de M√©tricas")
            # Gr√°fico de barras agrupadas
            fig = px.bar(
                performance_data.melt(id_vars=["Model"], var_name="M√©trica", value_name="Score"),
                x="Model",
                y="Score",
                color="M√©trica",
                barmode="group",
                title="Comparaci√≥n de Rendimiento entre Modelos",
                text="Score",  # Changed from text_auto to text
                color_discrete_map={
                    'Accuracy': COLORS['primary'],
                    'Precision': COLORS['secondary'],
                    'Recall': COLORS['accent'],
                    'F1-Score': COLORS['success']
                }
            )
            fig.update_layout(
                height=500,
                yaxis_title="Score",
                xaxis_title="Modelo",
                legend_title="M√©trica"
            )
            fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')  # This will now work
            st.plotly_chart(fig, use_container_width=True)
            
            # Gr√°fico de radar
            st.subheader("An√°lisis Multidimensional")
            
            fig_radar = go.Figure()
            
            metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
            
            for idx, row in performance_data.iterrows():
                model_name = row['Model']
                values = [row[metric] for metric in metrics]
                values.append(values[0])  # Cerrar el pol√≠gono
                
                fig_radar.add_trace(go.Scatterpolar(
                    r=values,
                    theta=metrics + [metrics[0]],
                    fill='toself',
                    name=model_name
                ))
            
            fig_radar.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )),
                showlegend=True,
                title="Perfil de Rendimiento por Modelo",
                height=500
            )
            
            st.plotly_chart(fig_radar, use_container_width=True)
        
        with tab2:
            st.subheader("M√©tricas Individuales por Modelo")
            
            for idx, row in performance_data.iterrows():
                with st.expander(f"üìä {row['Model']}", expanded=True):
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric(
                            "Accuracy",
                            f"{row['Accuracy']:.4f}",
                            delta=f"{(row['Accuracy'] - performance_data['Accuracy'].mean()):.4f}"
                        )
                    
                    with col2:
                        st.metric(
                            "Precision",
                            f"{row['Precision']:.4f}",
                            delta=f"{(row['Precision'] - performance_data['Precision'].mean()):.4f}"
                        )
                    
                    with col3:
                        st.metric(
                            "Recall",
                            f"{row['Recall']:.4f}",
                            delta=f"{(row['Recall'] - performance_data['Recall'].mean()):.4f}"
                        )
                    
                    with col4:
                        st.metric(
                            "F1-Score",
                            f"{row['F1-Score']:.4f}",
                            delta=f"{(row['F1-Score'] - performance_data['F1-Score'].mean()):.4f}"
                        )
        
        with tab3:
            st.subheader("Tabla Comparativa de Resultados")
            
            # Formatear DataFrame
            styled_df = performance_data.copy()
            for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
                styled_df[col] = styled_df[col].apply(lambda x: f"{x:.4f}")
            
            st.dataframe(
                styled_df,
                use_container_width=True,
                hide_index=True
            )
            
            # Identificar el mejor modelo
            best_model_idx = performance_data['F1-Score'].idxmax()
            best_model = performance_data.loc[best_model_idx, 'Model']
            best_f1 = performance_data.loc[best_model_idx, 'F1-Score']
            
            st.success(f"üèÜ **Mejor modelo seg√∫n F1-Score**: **{best_model}** ({best_f1:.4f})")
            
            # An√°lisis adicional
            st.markdown("---")
            st.subheader("üìä An√°lisis Estad√≠stico")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Promedios por M√©trica**")
                avg_stats = performance_data[['Accuracy', 'Precision', 'Recall', 'F1-Score']].mean()
                for metric, value in avg_stats.items():
                    st.markdown(f"- {metric}: `{value:.4f}`")
            
            with col2:
                st.markdown("**Desviaci√≥n Est√°ndar**")
                std_stats = performance_data[['Accuracy', 'Precision', 'Recall', 'F1-Score']].std()
                for metric, value in std_stats.items():
                    st.markdown(f"- {metric}: `{value:.4f}`")
    
    else:
        st.info("üëÜ Haz clic en el bot√≥n para calcular el rendimiento de los modelos en el conjunto de prueba.")
        
        st.markdown("""
        <div class="info-box">
            <h4>‚ÑπÔ∏è Sobre las M√©tricas</h4>
            <ul>
                <li><strong>Accuracy</strong>: Proporci√≥n de predicciones correctas sobre el total.</li>
                <li><strong>Precision</strong>: De las predicciones positivas, cu√°ntas fueron correctas.</li>
                <li><strong>Recall</strong>: De los casos positivos reales, cu√°ntos se detectaron.</li>
                <li><strong>F1-Score</strong>: Media arm√≥nica entre Precision y Recall.</li>
            </ul>
            <p>Todas las m√©tricas est√°n calculadas con promedio <code>macro</code>, 
            lo que significa que se calcula la m√©trica para cada clase y se promedian los resultados.</p>
        </div>
        """, unsafe_allow_html=True)

# Footer
st.markdown("---")
st.markdown(f"""
<div style="text-align: center; color: {COLORS['text_secondary']}; padding: 2rem 0;">
    <p style="font-weight: 600;">üìù Proyecto de Clasificaci√≥n de Argumentos</p>
    <p style="font-size: 0.9rem;">Desarrollado con Streamlit, PyTorch, y Transformers</p>
</div>
""", unsafe_allow_html=True)
