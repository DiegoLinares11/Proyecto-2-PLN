{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fa5cff-daa4-47c9-b399-0229b8aa734d",
   "metadata": {},
   "source": [
    "# Resultados parciales y visualizaciones estáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad8d570-6c46-4715-b77d-a56ec9153678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Repositories\\Proyecto-2-PLN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, DebertaV2Tokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffede144-d4ef-4880-b533-8bba9efc7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       essay_id  discourse_id discourse_type  \\\n",
      "0  007ACE74B050  0013cc385424           Lead   \n",
      "1  007ACE74B050  9704a709b505       Position   \n",
      "2  007ACE74B050  c22adee811b6          Claim   \n",
      "\n",
      "                                      discourse_text  \\\n",
      "0  Hi, i'm Isaac, i'm going to be writing about h...   \n",
      "1  On my perspective, I think that the face is a ...   \n",
      "2  I think that the face is a natural landform be...   \n",
      "\n",
      "                                discourse_text_clean  \\\n",
      "0  hi i'm isaac i'm going to be writing about how...   \n",
      "1  on my perspective i think that the face is a n...   \n",
      "2  i think that the face is a natural landform be...   \n",
      "\n",
      "                                          essay_text  \\\n",
      "0  Hi, i'm Isaac, i'm going to be writing about h...   \n",
      "1  Hi, i'm Isaac, i'm going to be writing about h...   \n",
      "2  Hi, i'm Isaac, i'm going to be writing about h...   \n",
      "\n",
      "                                    essay_text_clean discourse_effectiveness  \\\n",
      "0  hi i'm isaac i'm going to be writing about how...                Adequate   \n",
      "1  hi i'm isaac i'm going to be writing about how...                Adequate   \n",
      "2  hi i'm isaac i'm going to be writing about how...                Adequate   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "       essay_id  discourse_id discourse_type  \\\n",
      "0  D72CB1C11673  a261b6e14276           Lead   \n",
      "1  D72CB1C11673  5a88900e7dc1       Position   \n",
      "2  D72CB1C11673  9790d835736b          Claim   \n",
      "\n",
      "                                      discourse_text  \n",
      "0  Making choices in life can be very difficult. ...  \n",
      "1  Seeking multiple opinions can help a person ma...  \n",
      "2                     it can decrease stress levels   \n"
     ]
    }
   ],
   "source": [
    "# carga de los datos\n",
    "\n",
    "train_dataset = pd.read_csv(\"data/clean_train.csv\")\n",
    "test_dataset = pd.read_csv(\"data/clean_test.csv\")\n",
    "\n",
    "print(train_dataset.head(3))\n",
    "print(test_dataset.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fa73e-05ed-41bc-87fc-2e49ebf7605f",
   "metadata": {},
   "source": [
    "## SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94de2f38-201c-4aa9-8b8c-fbc895202996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      discourse_text  predicted_label\n",
      "0  Making choices in life can be very difficult. ...                1\n",
      "1  Seeking multiple opinions can help a person ma...                1\n",
      "2                     it can decrease stress levels                 1\n",
      "3             a great chance to learn something new                 2\n",
      "4               can be very helpful and beneficial.                 1\n"
     ]
    }
   ],
   "source": [
    "X_train = train_dataset[\"discourse_text_clean\"]\n",
    "y_train = train_dataset[\"label\"]\n",
    "\n",
    "# vectorización con TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    min_df=5,\n",
    "    norm='l2',\n",
    "    encoding='utf-8',\n",
    "    ngram_range=(1,2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(test_dataset[\"discourse_text\"])\n",
    "\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "test_predictions = svm.predict(X_test_tfidf)\n",
    "test_dataset[\"predicted_label\"] = test_predictions\n",
    "\n",
    "print(test_dataset[[\"discourse_text\", \"predicted_label\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21a8a1-7fe7-4fea-ab94-6258a6ed8cdc",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187994e7-bfba-406d-a16d-54fff43e5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset[\"discourse_text_clean\"].astype(str)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_dataset[\"discourse_effectiveness\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(list(test_dataset[\"discourse_text\"].astype(str)), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_ds = ArgumentDataset(train_encodings, y_train)\n",
    "val_ds = ArgumentDataset(val_encodings, y_val)\n",
    "test_ds = ArgumentDataset(test_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batc)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += len(batch[\"labels\"])\n",
    "    print(f\"Validation Accuracy:\")\n",
    "\n",
    "torch.save(model.state_dict(), \"distilbert_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07257f6f-ac6a-4df7-860b-2b12d2f50b8c",
   "metadata": {},
   "source": [
    "## DeBERTa-v3-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463877dd-522d-43ce-817d-db5171cf8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:   0%|                                                                                | 0/3677 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Epoch 1:   0%|                                                                     | 4/3677 [02:41<41:15:58, 40.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m     loss = outputs.loss\n\u001b[32m     55\u001b[39m     total_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     optimizer.step()\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Repositories\\Proyecto-2-PLN\\venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Repositories\\Proyecto-2-PLN\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Repositories\\Proyecto-2-PLN\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "X = train_dataset[\"discourse_text_clean\"].astype(str)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_dataset[\"discourse_effectiveness\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(list(test_dataset[\"discourse_text\"].astype(str)), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_ds = ArgumentDataset(train_encodings, y_train)\n",
    "val_ds = ArgumentDataset(val_encodings, y_val)\n",
    "test_ds = ArgumentDataset(test_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += len(batch[\"labels\"])\n",
    "    print(f\"Validation Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "save_dir = \"./\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "joblib.dump(label_encoder, f\"{save_dir}/label_encoder.pkl\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(test_ds, batch_size=8):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "pred_labels = label_encoder.inverse_transform(predictions)\n",
    "test_dataset[\"predicted_label\"] = pred_labels\n",
    "\n",
    "print(test_dataset[[\"discourse_text\", \"predicted_label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c67b0-923d-4e2b-87f1-d0a7ce4ef18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d23174-5140-481d-ace7-50934b0c53e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
